{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from icp import ICP_ROT\n",
    "from nc_score import pose_err\n",
    "from tqdm import tqdm\n",
    "import tools\n",
    "import rpmg\n",
    "import argparse\n",
    "from dataset.CameraPoseDataset import CameraPoseDatasetPred\n",
    "from colmap import read\n",
    "from rpr import find_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the relative pose\n",
    "def normalize_vector( v):\n",
    "    batch=v.shape[0]\n",
    "    v_mag = torch.sqrt(v.pow(2).sum(1))# batch\n",
    "    v_mag = torch.max(v_mag, torch.autograd.Variable(torch.FloatTensor([1e-8])))\n",
    "    v_mag = v_mag.view(batch,1).expand(batch,v.shape[1])\n",
    "    v = v/v_mag\n",
    "    return v\n",
    "\n",
    "def compute_quaternions_from_rotation_matrices(matrices):\n",
    "    batch=matrices.shape[0]\n",
    "    \n",
    "    w=torch.sqrt(torch.max(1.0 + matrices[:,0,0] + matrices[:,1,1] + matrices[:,2,2], torch.zeros(1))) / 2.0\n",
    "    w = torch.max (w , torch.autograd.Variable(torch.zeros(batch))+1e-8) #batch\n",
    "    w4 = 4.0 * w\n",
    "    x= (matrices[:,2,1] - matrices[:,1,2]) / w4\n",
    "    y= (matrices[:,0,2] - matrices[:,2,0]) / w4\n",
    "    z= (matrices[:,1,0] - matrices[:,0,1]) / w4\n",
    "    quats = torch.cat((w.view(batch,1), x.view(batch, 1),y.view(batch, 1), z.view(batch, 1) ), 1   )\n",
    "    quats = normalize_vector(quats)\n",
    "    return quats\n",
    "\n",
    "def compute_rotation_matrix_from_quaternion( quaternion, n_flag=True):\n",
    "    batch=quaternion.shape[0]\n",
    "    if n_flag:\n",
    "        quat = normalize_vector(quaternion)\n",
    "    else:\n",
    "        quat = quaternion\n",
    "    qw = quat[...,0].view(batch, 1)\n",
    "    qx = quat[...,1].view(batch, 1)\n",
    "    qy = quat[...,2].view(batch, 1)\n",
    "    qz = quat[...,3].view(batch, 1)\n",
    "\n",
    "    # Unit quaternion rotation matrices computatation  \n",
    "    xx = qx*qx\n",
    "    yy = qy*qy\n",
    "    zz = qz*qz\n",
    "    xy = qx*qy\n",
    "    xz = qx*qz\n",
    "    yz = qy*qz\n",
    "    xw = qx*qw\n",
    "    yw = qy*qw\n",
    "    zw = qz*qw\n",
    "\n",
    "    row0 = torch.cat((1-2*yy-2*zz, 2*xy - 2*zw, 2*xz + 2*yw), 1) #batch*3\n",
    "    row1 = torch.cat((2*xy+ 2*zw,  1-2*xx-2*zz, 2*yz-2*xw  ), 1) #batch*3\n",
    "    row2 = torch.cat((2*xz-2*yw,   2*yz+2*xw,   1-2*xx-2*yy), 1) #batch*3\n",
    "    \n",
    "    matrix = torch.cat((row0.view(batch, 1, 3), row1.view(batch,1,3), row2.view(batch,1,3)),1) #batch*3*3\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def compute_relative_pose(pose1, pose2):\n",
    "    '''\n",
    "    pose1 and pose2 are nx7 tensors, where the first 3 elements are the translation and the last 4 elements are the quaternion.\n",
    "    '''\n",
    "    # Assuming `tools.compute_rotation_matrix_from_quaternion` is correctly defined elsewhere\n",
    "    R1 = compute_rotation_matrix_from_quaternion(pose1[:, 3:])\n",
    "    R2 = compute_rotation_matrix_from_quaternion(pose2[:, 3:])\n",
    "    t1 = pose1[:, :3].unsqueeze(-1)  # Ensure t1 is nx3x1 for correct broadcasting in matrix operations\n",
    "    t2 = pose2[:, :3].unsqueeze(-1)  # Ensure t2 is nx3x1\n",
    "    # Compute the relative rotation\n",
    "    relative_R = R2.bmm(R1.transpose(1, 2))\n",
    "    \n",
    "    # Compute the relative translation\n",
    "    relative_t = t2 - relative_R.bmm(t1)\n",
    "\n",
    "    # Flatten relative_t back to nx3 for concatenation\n",
    "    relative_t = relative_t.squeeze(-1)\n",
    "\n",
    "    # For returning, you may want to convert relative_R back to quaternions and concatenate with relative_t\n",
    "    # Assuming `tools.compute_quaternion_from_rotation_matrix` is a function that converts rotation matrices to quaternions\n",
    "    relative_quaternions = compute_quaternions_from_rotation_matrices(relative_R)\n",
    "    relative_pose = torch.cat((relative_t, relative_quaternions), dim=1)\n",
    "\n",
    "    return relative_pose\n",
    "\n",
    "def compute_and_compare_pose(T_r, test_pose, gt_pose):\n",
    "    \"\"\"\n",
    "    Compute the final pose using the relative pose T_r and test_pose, \n",
    "    and compare it with the ground truth pose (gt_pose).\n",
    "    \n",
    "    :param T_r: numpy array of shape (4, 4), the relative transformation matrix\n",
    "    :param test_pose: torch.Tensor of shape (1, 7), the test pose in translation + quaternion format\n",
    "    :param gt_pose: numpy array of shape (7,), the ground truth pose in translation + quaternion format\n",
    "    :return: position error and orientation error\n",
    "    \"\"\"\n",
    "    # Convert quaternions to rotation matrices\n",
    "    R_test = compute_rotation_matrix_from_quaternion(test_pose[:, 3:], n_flag=True)\n",
    "\n",
    "    # Create transformation matrices for test_pose\n",
    "    T_test = torch.zeros((4, 4))\n",
    "    T_test[:3, :3] = R_test.squeeze(0)\n",
    "    T_test[:3, 3] = test_pose[:, :3].squeeze(0)\n",
    "    T_test[3, 3] = 1\n",
    "    \n",
    "    # Convert numpy T_r to tensor and apply relative transformation\n",
    "    T_final = torch.mm(torch.FloatTensor(T_r), T_test)\n",
    "\n",
    "    # Convert the final transformation matrix back to translation and quaternion format\n",
    "    t_final = T_final[:3, 3]\n",
    "    R_final = T_final[:3, :3]\n",
    "    quaternion_final = compute_quaternions_from_rotation_matrices(R_final.unsqueeze(0))\n",
    "\n",
    "    # Final pose in translation + quaternion format\n",
    "    final_pose = torch.cat((t_final, quaternion_final.squeeze(0)), 0).unsqueeze(0)\n",
    "\n",
    "    # Compare with gt_pose\n",
    "    gt_pose_tensor = torch.FloatTensor(gt_pose).unsqueeze(0)\n",
    "    posit_err, orient_err = pose_err(final_pose, gt_pose_tensor)\n",
    "\n",
    "    return posit_err.item(), orient_err.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.CameraPoseDataset import CameraPoseDatasetPred\n",
    "dataset_path = '/home/runyi/Data/7Scenes'\n",
    "cal_labels_file = '/home/runyi/Project/TBCP6D/dataset/7Scenes_0.5/abs_7scenes_pose.csv_chess_cal.csv_results.csv'\n",
    "test_labels_file = '/home/runyi/Project/TBCP6D/dataset/7Scenes_0.5/abs_7scenes_pose.csv_chess_test.csv_results.csv'\n",
    "cal_set = CameraPoseDatasetPred(dataset_path, cal_labels_file)\n",
    "test_set = CameraPoseDatasetPred(dataset_path, test_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2417, 7), (2417, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_set.poses.shape, cal_set.pred_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3534, 1.0696, 1.6345,  ..., 1.8173, 2.1381, 3.8093],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icp_rot = ICP_ROT(torch.tensor(cal_set.poses[:, 3:]), torch.tensor(cal_set.pred_poses[:, 3:]))\n",
    "icp_rot.compute_non_conformity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Function to find and match SURF features\n",
    "def find_and_match_features(image1, image2):\n",
    "    # Create SIFT object\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(image1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(image2, None)\n",
    "    \n",
    "    # BFMatcher with default params\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    \n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75*n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    return kp1, kp2, good_matches\n",
    "\n",
    "# Function to estimate essential matrix and recover pose\n",
    "def estimate_pose(kp1, kp2, matches, K):\n",
    "    points1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    E, mask = cv2.findEssentialMat(points1, points2, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, mask = cv2.recoverPose(E, points1, points2, K)\n",
    "    return R, t\n",
    "\n",
    "def find_poses(image1, image2):\n",
    "    kp1, kp2, good_matches = find_and_match_features(image1, image2)\n",
    "    K = np.array([[585, 0, 320],\n",
    "                  [0, 585, 240],\n",
    "                  [0, 0, 1]], dtype=np.float32)\n",
    "    R, t = estimate_pose(kp1, kp2, good_matches, K)\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3] = t.squeeze()  # Ensure t is correctly shaped\n",
    "    return T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runyi/Project/TBCP6D/icp.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.non_conformity_scores)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5275134464211833]\n",
      "[0.6474968969797269]\n",
      "[0.8134050475796442]\n",
      "[0.44352503103020274]\n",
      "[0.5755068266446007]\n",
      "[0.6218452627223832]\n",
      "[0.9743483657426562]\n",
      "[0.8585022755482002]\n",
      "[0.8692594124948283]\n",
      "[0.998345055854365]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/core/src/matrix.cpp:1173: error: (-13:Image step is wrong) The matrix is not continuous, thus its number of rows can not be changed in function 'reshape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m p_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, cal_img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cal_set\u001b[38;5;241m.\u001b[39mimgs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Assume find_poses computes the relative transformation matrix correctly\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     relative_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mfind_poses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_img\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     relative_R \u001b[38;5;241m=\u001b[39m relative_pose[:\u001b[38;5;241m3\u001b[39m, :\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     17\u001b[0m     relative_t \u001b[38;5;241m=\u001b[39m relative_pose[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mfind_poses\u001b[0;34m(image1, image2)\u001b[0m\n\u001b[1;32m     31\u001b[0m kp1, kp2, good_matches \u001b[38;5;241m=\u001b[39m find_and_match_features(image1, image2)\n\u001b[1;32m     32\u001b[0m K \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m585\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m320\u001b[39m],\n\u001b[1;32m     33\u001b[0m               [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m585\u001b[39m, \u001b[38;5;241m240\u001b[39m],\n\u001b[1;32m     34\u001b[0m               [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 35\u001b[0m R, t \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood_matches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m T \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     37\u001b[0m T[:\u001b[38;5;241m3\u001b[39m, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m R\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mestimate_pose\u001b[0;34m(kp1, kp2, matches, K)\u001b[0m\n\u001b[1;32m     25\u001b[0m points2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([kp2[m\u001b[38;5;241m.\u001b[39mtrainIdx]\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches])\n\u001b[1;32m     26\u001b[0m E, mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindEssentialMat(points1, points2, K, method\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mRANSAC, prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m _, R, t, mask \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecoverPose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m R, t\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/core/src/matrix.cpp:1173: error: (-13:Image step is wrong) The matrix is not continuous, thus its number of rows can not be changed in function 'reshape'\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "for idx, minibatch in enumerate(dataloader):\n",
    "    img = minibatch.get('img')[0].numpy()  # Assuming this retrieves the current image\n",
    "    pred_pose = minibatch.get('est_pose')  # Predicted pose from the model\n",
    "    # Convert quaternion part to rotation matrix\n",
    "    pred_R = compute_rotation_matrix_from_quaternion(pred_pose[:, 3:]).squeeze(0)\n",
    "    pred_t = pred_pose[:, :3].squeeze(0)\n",
    "    \n",
    "    pose_errs0, pose_errs1, pose_errs2, pose_errs3, pose_errs4 = [], [], [], [], []\n",
    "    cal_test_err_ori = []\n",
    "    cal_test_err_adj_ori = []\n",
    "    p_values = []\n",
    "    for idx, cal_img in enumerate(cal_set.imgs):\n",
    "        # Assume find_poses computes the relative transformation matrix correctly\n",
    "        relative_pose = torch.tensor(find_poses(img, cal_img))\n",
    "        relative_R = relative_pose[:3, :3]\n",
    "        relative_t = relative_pose[:3, 3]\n",
    "        # cal gt \n",
    "        # relative pose\n",
    "        # test pred pose\n",
    "        adjusted_R = relative_R.T @ pred_R\n",
    "        \n",
    "        adjusted_q = compute_quaternions_from_rotation_matrices(adjusted_R.unsqueeze(0))\n",
    "        adjusted_t = relative_R.T @ pred_t - relative_R.T @ relative_t\n",
    "        adjusted_v = torch.cat((adjusted_t, adjusted_q.squeeze(0)), 0).unsqueeze(0)\n",
    "        \n",
    "        adj_t_err, adj_o_err = pose_err(adjusted_v, torch.tensor(cal_set.poses[idx]).unsqueeze(0))\n",
    "\n",
    "        cal_test_err_adj_ori.append(adj_o_err.item())\n",
    "\n",
    "    cal_err_ori = icp_rot.get_non_conformity_scores()\n",
    "    \n",
    "    best_ori_error = torch.min(torch.tensor(cal_test_err_adj_ori))\n",
    "    p_value = (best_ori_error <= cal_err_ori).sum().item()\n",
    "    p_values.append(p_value / len(cal_err_ori))\n",
    "    print(p_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP6D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
