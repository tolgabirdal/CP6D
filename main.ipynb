{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from icp import ICP_ROT\n",
    "from nc_score import pose_err\n",
    "from tqdm import tqdm\n",
    "import tools\n",
    "import rpmg\n",
    "import argparse\n",
    "from dataset.CameraPoseDataset import CameraPoseDatasetPred\n",
    "from rpr import find_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the relative pose\n",
    "def normalize_vector( v):\n",
    "    batch=v.shape[0]\n",
    "    v_mag = torch.sqrt(v.pow(2).sum(1))# batch\n",
    "    v_mag = torch.max(v_mag, torch.autograd.Variable(torch.FloatTensor([1e-8])))\n",
    "    v_mag = v_mag.view(batch,1).expand(batch,v.shape[1])\n",
    "    v = v/v_mag\n",
    "    return v\n",
    "\n",
    "def compute_quaternions_from_rotation_matrices(matrices):\n",
    "    batch=matrices.shape[0]\n",
    "    \n",
    "    w=torch.sqrt(torch.max(1.0 + matrices[:,0,0] + matrices[:,1,1] + matrices[:,2,2], torch.zeros(1))) / 2.0\n",
    "    w = torch.max (w , torch.autograd.Variable(torch.zeros(batch))+1e-8) #batch\n",
    "    w4 = 4.0 * w\n",
    "    x= (matrices[:,2,1] - matrices[:,1,2]) / w4\n",
    "    y= (matrices[:,0,2] - matrices[:,2,0]) / w4\n",
    "    z= (matrices[:,1,0] - matrices[:,0,1]) / w4\n",
    "    quats = torch.cat((w.view(batch,1), x.view(batch, 1),y.view(batch, 1), z.view(batch, 1) ), 1   )\n",
    "    quats = normalize_vector(quats)\n",
    "    return quats\n",
    "\n",
    "def compute_rotation_matrix_from_quaternion( quaternion, n_flag=True):\n",
    "    batch=quaternion.shape[0]\n",
    "    if n_flag:\n",
    "        quat = normalize_vector(quaternion)\n",
    "    else:\n",
    "        quat = quaternion\n",
    "    qw = quat[...,0].view(batch, 1)\n",
    "    qx = quat[...,1].view(batch, 1)\n",
    "    qy = quat[...,2].view(batch, 1)\n",
    "    qz = quat[...,3].view(batch, 1)\n",
    "\n",
    "    # Unit quaternion rotation matrices computatation  \n",
    "    xx = qx*qx\n",
    "    yy = qy*qy\n",
    "    zz = qz*qz\n",
    "    xy = qx*qy\n",
    "    xz = qx*qz\n",
    "    yz = qy*qz\n",
    "    xw = qx*qw\n",
    "    yw = qy*qw\n",
    "    zw = qz*qw\n",
    "\n",
    "    row0 = torch.cat((1-2*yy-2*zz, 2*xy - 2*zw, 2*xz + 2*yw), 1) #batch*3\n",
    "    row1 = torch.cat((2*xy+ 2*zw,  1-2*xx-2*zz, 2*yz-2*xw  ), 1) #batch*3\n",
    "    row2 = torch.cat((2*xz-2*yw,   2*yz+2*xw,   1-2*xx-2*yy), 1) #batch*3\n",
    "    \n",
    "    matrix = torch.cat((row0.view(batch, 1, 3), row1.view(batch,1,3), row2.view(batch,1,3)),1) #batch*3*3\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def compute_relative_pose(pose1, pose2):\n",
    "    '''\n",
    "    pose1 and pose2 are nx7 tensors, where the first 3 elements are the translation and the last 4 elements are the quaternion.\n",
    "    '''\n",
    "    # Assuming `tools.compute_rotation_matrix_from_quaternion` is correctly defined elsewhere\n",
    "    R1 = compute_rotation_matrix_from_quaternion(pose1[:, 3:])\n",
    "    R2 = compute_rotation_matrix_from_quaternion(pose2[:, 3:])\n",
    "    t1 = pose1[:, :3].unsqueeze(-1)  # Ensure t1 is nx3x1 for correct broadcasting in matrix operations\n",
    "    t2 = pose2[:, :3].unsqueeze(-1)  # Ensure t2 is nx3x1\n",
    "    # Compute the relative rotation\n",
    "    relative_R = R2.bmm(R1.transpose(1, 2))\n",
    "    \n",
    "    # Compute the relative translation\n",
    "    relative_t = t2 - relative_R.bmm(t1)\n",
    "\n",
    "    # Flatten relative_t back to nx3 for concatenation\n",
    "    relative_t = relative_t.squeeze(-1)\n",
    "\n",
    "    # For returning, you may want to convert relative_R back to quaternions and concatenate with relative_t\n",
    "    # Assuming `tools.compute_quaternion_from_rotation_matrix` is a function that converts rotation matrices to quaternions\n",
    "    relative_quaternions = compute_quaternions_from_rotation_matrices(relative_R)\n",
    "    relative_pose = torch.cat((relative_t, relative_quaternions), dim=1)\n",
    "\n",
    "    return relative_pose\n",
    "\n",
    "def compute_and_compare_pose(T_r, test_pose, gt_pose):\n",
    "    \"\"\"\n",
    "    Compute the final pose using the relative pose T_r and test_pose, \n",
    "    and compare it with the ground truth pose (gt_pose).\n",
    "    \n",
    "    :param T_r: numpy array of shape (4, 4), the relative transformation matrix\n",
    "    :param test_pose: torch.Tensor of shape (1, 7), the test pose in translation + quaternion format\n",
    "    :param gt_pose: numpy array of shape (7,), the ground truth pose in translation + quaternion format\n",
    "    :return: position error and orientation error\n",
    "    \"\"\"\n",
    "    # Convert quaternions to rotation matrices\n",
    "    R_test = compute_rotation_matrix_from_quaternion(test_pose[:, 3:], n_flag=True)\n",
    "\n",
    "    # Create transformation matrices for test_pose\n",
    "    T_test = torch.zeros((4, 4))\n",
    "    T_test[:3, :3] = R_test.squeeze(0)\n",
    "    T_test[:3, 3] = test_pose[:, :3].squeeze(0)\n",
    "    T_test[3, 3] = 1\n",
    "    \n",
    "    # Convert numpy T_r to tensor and apply relative transformation\n",
    "    T_final = torch.mm(torch.FloatTensor(T_r), T_test)\n",
    "\n",
    "    # Convert the final transformation matrix back to translation and quaternion format\n",
    "    t_final = T_final[:3, 3]\n",
    "    R_final = T_final[:3, :3]\n",
    "    quaternion_final = compute_quaternions_from_rotation_matrices(R_final.unsqueeze(0))\n",
    "\n",
    "    # Final pose in translation + quaternion format\n",
    "    final_pose = torch.cat((t_final, quaternion_final.squeeze(0)), 0).unsqueeze(0)\n",
    "\n",
    "    # Compare with gt_pose\n",
    "    gt_pose_tensor = torch.FloatTensor(gt_pose).unsqueeze(0)\n",
    "    posit_err, orient_err = pose_err(final_pose, gt_pose_tensor)\n",
    "\n",
    "    return posit_err.item(), orient_err.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.CameraPoseDataset import CameraPoseDatasetPred\n",
    "dataset_path = '/home/runyi/Data/7Scenes'\n",
    "cal_labels_file = '/home/runyi/Project/TBCP6D/dataset/7Scenes_0.5/abs_7scenes_pose.csv_chess_cal.csv_results.csv'\n",
    "test_labels_file = '/home/runyi/Project/TBCP6D/dataset/7Scenes_0.5/abs_7scenes_pose.csv_chess_test.csv_results.csv'\n",
    "cal_set = CameraPoseDatasetPred(dataset_path, cal_labels_file)\n",
    "test_set = CameraPoseDatasetPred(dataset_path, test_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2417, 7), (2417, 7))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_set.poses.shape, cal_set.pred_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3534, 1.0696, 1.6345,  ..., 1.8173, 2.1381, 3.8093],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icp_rot = ICP_ROT(torch.tensor(cal_set.poses[:, 3:]), torch.tensor(cal_set.pred_poses[:, 3:]))\n",
    "icp_rot.compute_non_conformity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Function to find and match SURF features\n",
    "def find_and_match_features(image1, image2):\n",
    "    # Create SIFT object\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(image1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(image2, None)\n",
    "    \n",
    "    # BFMatcher with default params\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    \n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75*n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    return kp1, kp2, good_matches\n",
    "\n",
    "# Function to estimate essential matrix and recover pose\n",
    "def estimate_pose(kp1, kp2, matches, K):\n",
    "    points1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    E, mask = cv2.findEssentialMat(points1, points2, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, mask = cv2.recoverPose(E, points1, points2, K)\n",
    "    return R, t\n",
    "\n",
    "def find_poses(image1, image2):\n",
    "    kp1, kp2, good_matches = find_and_match_features(image1, image2)\n",
    "    K = np.array([[585, 0, 320],\n",
    "                  [0, 585, 240],\n",
    "                  [0, 0, 1]], dtype=np.float32)\n",
    "    R, t = estimate_pose(kp1, kp2, good_matches, K)\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3] = t.squeeze()  # Ensure t is correctly shaped\n",
    "    return T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3723534dad44dfbe2197b1f0e12619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.468002795298486 0.6876909731023402 5.612451309633198 3.9345160974890665\n",
      "0.47171771092498554 0.6292939996193537 5.472318325396608 2.767206653701752\n",
      "0.47180939790871196 0.6234745133167432 5.633418987270219 2.8004720421336002\n",
      "0.4726175504811338 0.6712018291125184 5.61653517336346 3.6504108245301534\n",
      "0.4733220965802733 0.6499754370931642 5.62926274980952 3.34443880677191\n",
      "0.4715396107964508 0.6662515344951502 5.611286257145665 3.564053990532679\n",
      "0.47203121821776445 0.6387795159132522 5.524266570993478 2.971067552047768\n",
      "0.4724456047111646 0.7312202906345413 5.359687957273786 4.718441595996223\n",
      "0.4690501716699451 0.6435635435119023 5.154753107166995 3.317676738971387\n",
      "0.4622852165348679 0.6464069169862703 5.428869515220478 3.056308445697282\n",
      "0.4581219864092347 0.6667712969001964 5.494086120752482 3.8200028954936385\n",
      "0.45191864463792986 0.5919539396348936 5.1294849563963805 1.89146174880846\n",
      "0.44845797901781687 0.6478982642848345 5.252769137110474 3.694513419880115\n",
      "0.44583624822807855 0.6888266914864912 5.3582878463680395 4.069295209676317\n",
      "0.4443757981530874 0.6694158011901772 5.185715341492775 4.227713982836945\n",
      "0.43760972339910803 0.7140247661451147 5.15692955523263 5.097175632371877\n",
      "0.4267423481286599 0.6482192721924909 5.301686243662893 4.503520242961042\n",
      "0.42362195349255355 0.7118806239314035 5.3682077318795764 5.665487242429222\n",
      "0.41996017415516507 0.6183956651798962 5.368267537217854 3.6233987397998058\n",
      "0.41701470752961467 0.6132948008077352 5.535920809796115 3.1002904733973105\n",
      "0.40988675851685424 0.6559611804785612 5.664370045111814 3.865426205990859\n",
      "0.4062814506961618 0.6405809752169715 6.1038635299035136 3.7778736743588026\n",
      "0.40206118334099783 0.6830467780927075 6.246742273136975 5.129398137688082\n",
      "0.39214043141964483 0.6160993551470745 6.581613976893762 2.7744068585775548\n",
      "0.3809668750626143 0.6820293056890517 6.807659752429656 5.778780715010556\n",
      "0.37895338758219166 0.6876295446619328 6.82891709645639 5.813258171760524\n",
      "0.3769624192546623 0.6687242171742434 6.8084593485323674 5.623277693146728\n",
      "0.374684266698443 0.6339733276101369 6.890810756577837 4.105852077818674\n",
      "0.3695183318740023 0.7320862969766005 7.341596793625649 7.0093025991811615\n",
      "0.3709287491376598 0.6450558140080179 7.287145703981585 4.414029322350517\n",
      "0.3657563709388296 0.6402919589188384 7.658930357914415 3.432279242840597\n",
      "0.36382588872985827 0.6271810459486378 7.893050238300384 1.9808448391833196\n",
      "0.35780326572445703 0.7288193922441828 8.232067194170284 7.348698887522671\n",
      "0.35281294124677387 0.6783277046840392 8.813409277656516 6.030743191816739\n",
      "0.3438835990287533 0.6598082770534768 10.362145381027727 4.061591359124694\n",
      "0.33989852212793215 0.7208987762100256 10.504258213345677 9.078140372500723\n",
      "0.3361306802899399 0.7001182848902923 11.72891494741972 5.318535077656675\n",
      "0.33738995168803315 0.6912155180958012 12.551932653776293 8.141555183270205\n",
      "0.33888418487948896 0.7231933632536741 12.43110881422163 5.630285581643276\n",
      "0.3412661221059186 0.6794353055732604 12.643222340364568 8.4829449306483\n",
      "0.3402904203985585 0.7166859802747241 12.857490498634968 6.267919961981452\n",
      "0.34336564268137415 0.7196610490930758 13.230149574476313 7.120011412303497\n",
      "0.34445722810312157 0.7320227237253868 13.28788787450776 6.224125229740598\n",
      "0.34876958761281446 0.7196417955809025 13.47297119096946 6.632594152525052\n",
      "0.3550741524819149 0.7371368877162544 13.581053474415286 6.253001501103108\n",
      "0.3634479052554518 0.7290232825285612 14.084773251900064 7.087485826958385\n",
      "0.3713753292335149 0.7466388026416679 14.482016784575722 6.3800234399193725\n",
      "0.37443360778463497 0.7928743083006631 14.634950667967598 4.873381140660112\n",
      "0.380323175794974 0.7667477813836092 14.924108881585349 5.740634489062182\n",
      "0.38446390594474605 0.744640062213053 14.997069465350712 6.27725663149967\n",
      "0.39603362277131016 0.7608939647535978 15.394380560414552 6.09381990736878\n",
      "0.4003264593570878 0.7695380075307624 15.505022878599016 6.001329930777248\n",
      "0.4198113816753349 0.7234874232199633 15.650759925585442 7.048992703809673\n",
      "0.43270348646410406 0.713525675247337 15.893311687345488 7.387680592799662\n",
      "0.43724458104642344 0.7099760132472989 15.694127998952382 6.72981994846837\n",
      "0.4669883716596568 0.7118411036202893 16.178289669860952 7.054346498734548\n",
      "0.47301239094139785 0.6861997237198914 16.230333379950668 7.585049358244057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m pose_errs0, pose_errs1, pose_errs2, pose_errs3, pose_errs4 \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, cal_img \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(cal_set\u001b[38;5;241m.\u001b[39mimgs)):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Assume find_poses computes the relative transformation matrix correctly\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     relative_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mfind_poses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_img\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     relative_R \u001b[38;5;241m=\u001b[39m relative_pose[:\u001b[38;5;241m3\u001b[39m, :\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     15\u001b[0m     relative_t \u001b[38;5;241m=\u001b[39m relative_pose[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m, in \u001b[0;36mfind_poses\u001b[0;34m(image1, image2)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_poses\u001b[39m(image1, image2):\n\u001b[0;32m---> 31\u001b[0m     kp1, kp2, good_matches \u001b[38;5;241m=\u001b[39m \u001b[43mfind_and_match_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     K \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m585\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m320\u001b[39m],\n\u001b[1;32m     33\u001b[0m                   [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m585\u001b[39m, \u001b[38;5;241m240\u001b[39m],\n\u001b[1;32m     34\u001b[0m                   [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     35\u001b[0m     R, t \u001b[38;5;241m=\u001b[39m estimate_pose(kp1, kp2, good_matches, K)\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mfind_and_match_features\u001b[0;34m(image1, image2)\u001b[0m\n\u001b[1;32m      6\u001b[0m sift \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mSIFT_create()\n\u001b[1;32m      7\u001b[0m kp1, des1 \u001b[38;5;241m=\u001b[39m sift\u001b[38;5;241m.\u001b[39mdetectAndCompute(image1, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m kp2, des2 \u001b[38;5;241m=\u001b[39m \u001b[43msift\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectAndCompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# BFMatcher with default params\u001b[39;00m\n\u001b[1;32m     11\u001b[0m bf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mBFMatcher()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "for idx, minibatch in enumerate(dataloader):\n",
    "    img = minibatch.get('img')[0].numpy()  # Assuming this retrieves the current image\n",
    "    pred_pose = minibatch.get('est_pose')  # Predicted pose from the model\n",
    "    # Convert quaternion part to rotation matrix\n",
    "    pred_R = compute_rotation_matrix_from_quaternion(pred_pose[:, 3:]).squeeze(0)\n",
    "    pred_t = pred_pose[:, :3].squeeze(0)\n",
    "    \n",
    "    pose_errs0, pose_errs1, pose_errs2, pose_errs3, pose_errs4 = [], [], [], [], []\n",
    "    \n",
    "    for idx, cal_img in tqdm(enumerate(cal_set.imgs)):\n",
    "        # Assume find_poses computes the relative transformation matrix correctly\n",
    "        relative_pose = torch.tensor(find_poses(img, cal_img))\n",
    "        relative_R = relative_pose[:3, :3]\n",
    "        relative_t = relative_pose[:3, 3]\n",
    "        # cal gt \n",
    "        # relative pose\n",
    "        # test pred pose\n",
    "        adjusted_R = relative_R.T @ pred_R\n",
    "        \n",
    "        adjusted_q = compute_quaternions_from_rotation_matrices(adjusted_R.unsqueeze(0))\n",
    "        adjusted_t = relative_R.T @ pred_t - relative_R.T @ relative_t\n",
    "        adjusted_v = torch.cat((adjusted_t, adjusted_q.squeeze(0)), 0).unsqueeze(0)\n",
    "        \n",
    "        adj_t_err, adj_o_err = pose_err(adjusted_v, torch.tensor(cal_set.poses[idx]).unsqueeze(0))\n",
    "        p0, o0 = pose_err(pred_pose, torch.tensor(cal_set.poses[idx]).unsqueeze(0))\n",
    "        print(p0.item(), adj_t_err.item(), o0.item(), adj_o_err.item())\n",
    "    #     pose_errs0.append(o0.item()); pose_errs1.append(orient_err1.item()); pose_errs2.append(orient_err2.item()); pose_errs3.append(orient_err3.item()); pose_errs4.append(orient_err4.item())\n",
    "    #     if idx == 200:\n",
    "    #         break\n",
    "    # print(np.mean(pose_errs0), np.mean(pose_errs1), np.mean(pose_errs2), np.mean(pose_errs3), np.mean(pose_errs4))\n",
    "    break\n",
    "    #     relative_R = relative_pose[:3, :3]\n",
    "    #     relative_t = relative_pose[:3, 3]\n",
    "    #     # Compute the adjusted pose\n",
    "    #     adjusted_R = pred_R @ relative_R.T\n",
    "    #     adjusted_t = relative_R.bmm(pred_t.unsqueeze(-1)) + relative_t.unsqueeze(-1)\n",
    "    #     adjusted_quaternions = compute_quaternions_from_rotation_matrices(adjusted_R)\n",
    "    #     adjusted_pose_vector = torch.cat((adjusted_t.squeeze(2), adjusted_quaternions), dim=1)\n",
    "        \n",
    "    #     cal_gt_pose = torch.tensor(cal_set.poses[idx], dtype=torch.float64).unsqueeze(0)\n",
    "    #     # Compute positional and orientation errors\n",
    "    #     posit_err, orient_err = pose_err(adjusted_pose_vector, cal_gt_pose)\n",
    "    #     print(pose_err(torch.tensor(pred_pose).to(torch.float64), cal_gt_pose))\n",
    "    #     print(posit_err.item(), orient_err.item()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "icp_rot = ICP_ROT(torch.tensor(cal_set.poses[:, 3:]), torch.tensor(cal_set.pred_poses[:, 3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3534, 1.0696, 1.6345,  ..., 1.8173, 2.1381, 3.8093],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icp_rot.compute_non_conformity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9584651125763513"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "percentile_95 = np.percentile(icp_rot.non_conformity_scores, 50)\n",
    "percentile_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv_contrib/modules/xfeatures2d/src/surf.cpp:1026: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxfeatures2d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSURF_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv_contrib/modules/xfeatures2d/src/surf.cpp:1026: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.xfeatures2d.SURF_create(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0316476 , -0.76844001, -0.69450545,  0.99021727, -0.13535483,\n",
       "        0.02634406, -0.02132878])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP6D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
