{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from icp import ICP_ROT\n",
    "from nc_score import pose_err\n",
    "from tqdm import tqdm\n",
    "import tools\n",
    "import rpmg\n",
    "import argparse\n",
    "from dataset.CameraPoseDataset import CameraPoseDatasetPred\n",
    "from rpr import find_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the relative pose\n",
    "def normalize_vector( v):\n",
    "    batch=v.shape[0]\n",
    "    v_mag = torch.sqrt(v.pow(2).sum(1))# batch\n",
    "    v_mag = torch.max(v_mag, torch.autograd.Variable(torch.FloatTensor([1e-8])))\n",
    "    v_mag = v_mag.view(batch,1).expand(batch,v.shape[1])\n",
    "    v = v/v_mag\n",
    "    return v\n",
    "\n",
    "def compute_quaternions_from_rotation_matrices(matrices):\n",
    "    batch=matrices.shape[0]\n",
    "    \n",
    "    w=torch.sqrt(torch.max(1.0 + matrices[:,0,0] + matrices[:,1,1] + matrices[:,2,2], torch.zeros(1))) / 2.0\n",
    "    w = torch.max (w , torch.autograd.Variable(torch.zeros(batch))+1e-8) #batch\n",
    "    w4 = 4.0 * w\n",
    "    x= (matrices[:,2,1] - matrices[:,1,2]) / w4\n",
    "    y= (matrices[:,0,2] - matrices[:,2,0]) / w4\n",
    "    z= (matrices[:,1,0] - matrices[:,0,1]) / w4\n",
    "    quats = torch.cat((w.view(batch,1), x.view(batch, 1),y.view(batch, 1), z.view(batch, 1) ), 1   )\n",
    "    quats = normalize_vector(quats)\n",
    "    return quats\n",
    "\n",
    "def compute_rotation_matrix_from_quaternion( quaternion, n_flag=True):\n",
    "    batch=quaternion.shape[0]\n",
    "    if n_flag:\n",
    "        quat = normalize_vector(quaternion)\n",
    "    else:\n",
    "        quat = quaternion\n",
    "    qw = quat[...,0].view(batch, 1)\n",
    "    qx = quat[...,1].view(batch, 1)\n",
    "    qy = quat[...,2].view(batch, 1)\n",
    "    qz = quat[...,3].view(batch, 1)\n",
    "\n",
    "    # Unit quaternion rotation matrices computatation  \n",
    "    xx = qx*qx\n",
    "    yy = qy*qy\n",
    "    zz = qz*qz\n",
    "    xy = qx*qy\n",
    "    xz = qx*qz\n",
    "    yz = qy*qz\n",
    "    xw = qx*qw\n",
    "    yw = qy*qw\n",
    "    zw = qz*qw\n",
    "\n",
    "    row0 = torch.cat((1-2*yy-2*zz, 2*xy - 2*zw, 2*xz + 2*yw), 1) #batch*3\n",
    "    row1 = torch.cat((2*xy+ 2*zw,  1-2*xx-2*zz, 2*yz-2*xw  ), 1) #batch*3\n",
    "    row2 = torch.cat((2*xz-2*yw,   2*yz+2*xw,   1-2*xx-2*yy), 1) #batch*3\n",
    "    \n",
    "    matrix = torch.cat((row0.view(batch, 1, 3), row1.view(batch,1,3), row2.view(batch,1,3)),1) #batch*3*3\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def compute_relative_pose(pose1, pose2):\n",
    "    '''\n",
    "    pose1 and pose2 are nx7 tensors, where the first 3 elements are the translation and the last 4 elements are the quaternion.\n",
    "    '''\n",
    "    # Assuming `tools.compute_rotation_matrix_from_quaternion` is correctly defined elsewhere\n",
    "    R1 = compute_rotation_matrix_from_quaternion(pose1[:, 3:])\n",
    "    R2 = compute_rotation_matrix_from_quaternion(pose2[:, 3:])\n",
    "    t1 = pose1[:, :3].unsqueeze(-1)  # Ensure t1 is nx3x1 for correct broadcasting in matrix operations\n",
    "    t2 = pose2[:, :3].unsqueeze(-1)  # Ensure t2 is nx3x1\n",
    "    # Compute the relative rotation\n",
    "    relative_R = R2.bmm(R1.transpose(1, 2))\n",
    "    \n",
    "    # Compute the relative translation\n",
    "    relative_t = t2 - relative_R.bmm(t1)\n",
    "\n",
    "    # Flatten relative_t back to nx3 for concatenation\n",
    "    relative_t = relative_t.squeeze(-1)\n",
    "\n",
    "    # For returning, you may want to convert relative_R back to quaternions and concatenate with relative_t\n",
    "    # Assuming `tools.compute_quaternion_from_rotation_matrix` is a function that converts rotation matrices to quaternions\n",
    "    relative_quaternions = compute_quaternions_from_rotation_matrices(relative_R)\n",
    "    relative_pose = torch.cat((relative_t, relative_quaternions), dim=1)\n",
    "\n",
    "    return relative_pose\n",
    "\n",
    "def compute_and_compare_pose(T_r, test_pose, gt_pose):\n",
    "    \"\"\"\n",
    "    Compute the final pose using the relative pose T_r and test_pose, \n",
    "    and compare it with the ground truth pose (gt_pose).\n",
    "    \n",
    "    :param T_r: numpy array of shape (4, 4), the relative transformation matrix\n",
    "    :param test_pose: torch.Tensor of shape (1, 7), the test pose in translation + quaternion format\n",
    "    :param gt_pose: numpy array of shape (7,), the ground truth pose in translation + quaternion format\n",
    "    :return: position error and orientation error\n",
    "    \"\"\"\n",
    "    # Convert quaternions to rotation matrices\n",
    "    R_test = compute_rotation_matrix_from_quaternion(test_pose[:, 3:], n_flag=True)\n",
    "\n",
    "    # Create transformation matrices for test_pose\n",
    "    T_test = torch.zeros((4, 4))\n",
    "    T_test[:3, :3] = R_test.squeeze(0)\n",
    "    T_test[:3, 3] = test_pose[:, :3].squeeze(0)\n",
    "    T_test[3, 3] = 1\n",
    "    \n",
    "    # Convert numpy T_r to tensor and apply relative transformation\n",
    "    T_final = torch.mm(torch.FloatTensor(T_r), T_test)\n",
    "\n",
    "    # Convert the final transformation matrix back to translation and quaternion format\n",
    "    t_final = T_final[:3, 3]\n",
    "    R_final = T_final[:3, :3]\n",
    "    quaternion_final = compute_quaternions_from_rotation_matrices(R_final.unsqueeze(0))\n",
    "\n",
    "    # Final pose in translation + quaternion format\n",
    "    final_pose = torch.cat((t_final, quaternion_final.squeeze(0)), 0).unsqueeze(0)\n",
    "\n",
    "    # Compare with gt_pose\n",
    "    gt_pose_tensor = torch.FloatTensor(gt_pose).unsqueeze(0)\n",
    "    posit_err, orient_err = pose_err(final_pose, gt_pose_tensor)\n",
    "\n",
    "    return posit_err.item(), orient_err.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.CameraPoseDataset import CameraPoseDatasetPred\n",
    "dataset_path = '/home/runyi/Data/7Scenes'\n",
    "cal_labels_file = '/home/runyi/Project/TBCP6D/dataset/7Scenes_0.5/abs_7scenes_pose.csv_chess_cal.csv_results.csv'\n",
    "test_labels_file = '/home/runyi/Project/TBCP6D/dataset/7Scenes_0.5/abs_7scenes_pose.csv_chess_test.csv_results.csv'\n",
    "cal_set = CameraPoseDatasetPred(dataset_path, cal_labels_file)\n",
    "test_set = CameraPoseDatasetPred(dataset_path, test_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2417, 7), (2417, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_set.poses.shape, cal_set.pred_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "icp_rot = ICP_ROT(torch.tensor(cal_set.poses[:, 3:]), torch.tensor(cal_set.pred_poses[:, 3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m relative_R \u001b[38;5;241m=\u001b[39m relative_T[:, :\u001b[38;5;241m3\u001b[39m, :\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     13\u001b[0m relative_t \u001b[38;5;241m=\u001b[39m relative_T[:, :\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m test_pred_R, test_pred_t \u001b[38;5;241m=\u001b[39m \u001b[43mtest_pred_pose\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m, test_pred_pose[:, :\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     17\u001b[0m compute_and_compare_pose(relative_T, test_pred_pose, cal_pose)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "cal_pose = cal_set.poses[0]\n",
    "cal_img = cal_set.imgs[0]\n",
    "cal_pred_pose = torch.tensor(cal_set.pred_poses[0]).unsqueeze(0)\n",
    "cal_img_path = cal_set.img_paths[0]\n",
    "\n",
    "test_pose = test_set.poses[0]\n",
    "test_img = test_set.imgs[0]\n",
    "test_pred_pose = torch.tensor(test_set.pred_poses[0]).unsqueeze(0)\n",
    "test_img_path = test_set.img_paths[0]\n",
    "\n",
    "relative_T = torch.tensor(find_poses(cal_img, test_img)).unsqueeze(0)\n",
    "relative_R = relative_T[:, :3, :3]\n",
    "relative_t = relative_T[:, :3, 3]\n",
    "\n",
    "test_pred_R, test_pred_t = test_pred_pose[:, :3, :3], test_pred_pose[:, :3, 3]\n",
    "\n",
    "compute_and_compare_pose(relative_T, test_pred_pose, cal_pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9977,  0.0351,  0.0579],\n",
      "         [-0.0494,  0.9624,  0.2669],\n",
      "         [-0.0464, -0.2692,  0.9620]]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m relative_t \u001b[38;5;241m=\u001b[39m relative_pose[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute the adjusted pose\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m adjusted_R \u001b[38;5;241m=\u001b[39m pred_R\u001b[38;5;241m.\u001b[39mbmm(\u001b[43mrelative_R\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m adjusted_t \u001b[38;5;241m=\u001b[39m relative_R\u001b[38;5;241m.\u001b[39mbmm(pred_t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m relative_t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m adjusted_quaternions \u001b[38;5;241m=\u001b[39m compute_quaternions_from_rotation_matrices(adjusted_R)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "for idx, minibatch in enumerate(dataloader):\n",
    "    img = minibatch.get('img')[0].numpy()  # Assuming this retrieves the current image\n",
    "    pred_pose = minibatch.get('est_pose')  # Predicted pose from the model\n",
    "    # Convert quaternion part to rotation matrix\n",
    "    pred_R = compute_rotation_matrix_from_quaternion(pred_pose[:, 3:]).squeeze(0)\n",
    "    pred_t = pred_pose[:, :3].squeeze(0)\n",
    "    \n",
    "    for idx, cal_img in enumerate(cal_set.imgs):\n",
    "        # Assume find_poses computes the relative transformation matrix correctly\n",
    "        relative_pose = torch.tensor(find_poses(cal_img, img))\n",
    "        relative_R = relative_pose[:3, :3]\n",
    "        relative_t = relative_pose[:3, 3]\n",
    "        # Compute the adjusted pose\n",
    "        adjusted_R = pred_R @ relative_R.T\n",
    "        adjusted_t = relative_R.bmm(pred_t.unsqueeze(-1)) + relative_t.unsqueeze(-1)\n",
    "        adjusted_quaternions = compute_quaternions_from_rotation_matrices(adjusted_R)\n",
    "        adjusted_pose_vector = torch.cat((adjusted_t.squeeze(2), adjusted_quaternions), dim=1)\n",
    "        \n",
    "        cal_gt_pose = torch.tensor(cal_set.poses[idx], dtype=torch.float64).unsqueeze(0)\n",
    "        # Compute positional and orientation errors\n",
    "        posit_err, orient_err = pose_err(adjusted_pose_vector, cal_gt_pose)\n",
    "        print(pose_err(torch.tensor(pred_pose).to(torch.float64), cal_gt_pose))\n",
    "        print(posit_err.item(), orient_err.item()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "icp_rot = ICP_ROT(torch.tensor(cal_set.poses[:, 3:]), torch.tensor(cal_set.pred_poses[:, 3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3534, 1.0696, 1.6345,  ..., 1.8173, 2.1381, 3.8093],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icp_rot.compute_non_conformity_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9584651125763513"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "percentile_95 = np.percentile(icp_rot.non_conformity_scores, 50)\n",
    "percentile_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0316476 , -0.76844001, -0.69450545,  0.99021727, -0.13535483,\n",
       "        0.02634406, -0.02132878])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP6D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
